name: Regression Testing

on:
  push:
    branches: [main, develop]
    paths:
      - "src/**"
      - "tests/**"
      - ".github/workflows/regression.yml"
  pull_request:
    branches: [main]
  schedule:
    - cron: "0 0 * * 3" # Weekly on Wednesday
  workflow_dispatch: # Allow manual trigger
    inputs:
      test_scope:
        description: "Regression test scope"
        required: true
        type: choice
        options:
          - all
          - core
          - features
          - performance
          - historical

jobs:
  core-regression:
    name: Core Functionality
    if: github.event.inputs.test_scope == 'core' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.8"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-regression pytest-historic

      - name: Run core regression tests
        run: |
          pytest tests/regression/test_core.py \
            --regression \
            --historic-data core_history.json \
            --junitxml=core-results.xml

      - name: Compare with baseline
        run: |
          python scripts/regression/compare_baseline.py \
            --current core-results.xml \
            --baseline baseline/core-baseline.xml \
            --output core-comparison.json

      - name: Generate core report
        run: |
          {
            echo "# Core Regression Report"
            echo "## Test Results"
            python scripts/regression/format_results.py core-results.xml
            echo "## Baseline Comparison"
            cat core-comparison.json | jq -r '.differences[] | "- " + .'
          } > core-report.md

  feature-regression:
    name: Feature Testing
    if: github.event.inputs.test_scope == 'features' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Run feature regression tests
        run: |
          pytest tests/regression/test_features.py \
            --regression \
            --historic-data feature_history.json \
            --junitxml=feature-results.xml

      - name: Check feature parity
        run: |
          python scripts/regression/check_feature_parity.py \
            --results feature-results.xml \
            --features features.json \
            --output parity-check.json

      - name: Generate feature report
        run: |
          {
            echo "# Feature Regression Report"
            echo "## Test Results"
            python scripts/regression/format_results.py feature-results.xml
            echo "## Feature Parity"
            cat parity-check.json | jq -r '.issues[] | "- " + .'
          } > feature-report.md

  performance-regression:
    name: Performance Testing
    if: github.event.inputs.test_scope == 'performance' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Run performance regression tests
        run: |
          pytest tests/regression/test_performance.py \
            --benchmark-only \
            --benchmark-json perf-results.json

      - name: Analyze performance trends
        run: |
          python scripts/regression/analyze_performance.py \
            --current perf-results.json \
            --history performance_history.json \
            --output performance-analysis.json

      - name: Generate performance report
        run: |
          {
            echo "# Performance Regression Report"
            echo "## Benchmark Results"
            cat perf-results.json | jq -r '.benchmarks[] | "- " + .name + ": " + (.stats.mean | tostring) + "s"'
            echo "## Performance Trends"
            cat performance-analysis.json | jq -r '.trends[] | "- " + .'
          } > performance-report.md

  historical-regression:
    name: Historical Analysis
    if: github.event.inputs.test_scope == 'historical' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Analyze historical data
        run: |
          python scripts/regression/analyze_history.py \
            --data regression_history.json \
            --output historical-analysis.json

      - name: Check for patterns
        run: |
          python scripts/regression/find_patterns.py \
            --data historical-analysis.json \
            --output patterns.json

      - name: Generate historical report
        run: |
          {
            echo "# Historical Regression Report"
            echo "## Analysis Results"
            cat historical-analysis.json | jq -r '.findings[] | "- " + .'
            echo "## Identified Patterns"
            cat patterns.json | jq -r '.patterns[] | "- " + .'
          } > historical-report.md

  report:
    name: Generate Report
    needs:
      [
        core-regression,
        feature-regression,
        performance-regression,
        historical-regression,
      ]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Combine reports
        run: |
          {
            echo "# Regression Test Report"
            
            echo "## Core Functionality"
            cat core-report.md
            
            echo "## Feature Testing"
            cat feature-report.md
            
            echo "## Performance Testing"
            cat performance-report.md
            
            echo "## Historical Analysis"
            cat historical-report.md
            
            echo "## Summary"
            echo "- Core Tests: ${{ needs.core-regression.result }}"
            echo "- Feature Tests: ${{ needs.feature-regression.result }}"
            echo "- Performance Tests: ${{ needs.performance-regression.result }}"
            echo "- Historical Analysis: ${{ needs.historical-regression.result }}"
          } > regression-report.md

      - name: Create issue for regressions
        if: contains(needs.*.result, 'failure')
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('regression-report.md', 'utf8');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '⚠️ Regression Detected',
              body: report,
              labels: ['regression', 'bug', 'high-priority']
            });

      - name: Update metrics
        run: |
          {
            echo "regression_tests_total $(date +%s)"
            echo "regressions_found $(echo '${{ needs.*.result }}' | grep -c failure || true)"
          } > regression_metrics.txt

          curl -X POST ${{ secrets.PROMETHEUS_PUSHGATEWAY }}/metrics/job/regression_testing \
            --data-binary "@regression_metrics.txt"

      - name: Notify team
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          title: "Regression Tests Complete"
          description: |
            Status: ${{ contains(needs.*.result, 'failure') && '❌ Regressions Found' || '✅ No Regressions' }}

            Check the workflow run for detailed results.
          color: ${{ contains(needs.*.result, 'failure') && '0xff0000' || '0x00ff00' }}
          username: "Regression Bot"
