name: Benchmark Testing

on:
  push:
    branches: [main, develop]
    paths:
      - "src/**"
      - "tests/benchmarks/**"
      - ".github/workflows/benchmark.yml"
  pull_request:
    branches: [main]
  schedule:
    - cron: "0 0 * * 1" # Weekly on Monday
  workflow_dispatch: # Allow manual trigger
    inputs:
      benchmark_type:
        description: "Type of benchmark"
        required: true
        type: choice
        options:
          - all
          - performance
          - memory
          - latency
          - throughput
      iterations:
        description: "Number of iterations"
        required: true
        type: number
        default: 1000

jobs:
  prepare:
    name: Prepare Benchmark Environment
    runs-on: ubuntu-latest
    outputs:
      test_id: ${{ steps.setup.outputs.test_id }}

    steps:
      - name: Generate test ID
        id: setup
        run: |
          echo "test_id=$(date +%Y%m%d_%H%M%S)" >> $GITHUB_OUTPUT

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.8"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-benchmark memory_profiler py-spy scalene

  performance:
    name: Performance Benchmarks
    needs: prepare
    if: github.event.inputs.benchmark_type == 'performance' || github.event.inputs.benchmark_type == 'all'
    runs-on: ubuntu-latest

    steps:
      - name: Run command benchmarks
        run: |
          pytest tests/benchmarks/test_command_perf.py \
            --benchmark-only \
            --benchmark-json command_benchmarks.json \
            --benchmark-min-rounds ${{ github.event.inputs.iterations || 1000 }}

      - name: Run processing benchmarks
        run: |
          pytest tests/benchmarks/test_processing_perf.py \
            --benchmark-only \
            --benchmark-json processing_benchmarks.json \
            --benchmark-min-rounds ${{ github.event.inputs.iterations || 1000 }}

      - name: Generate performance report
        run: |
          {
            echo "# Performance Benchmark Report"
            echo "## Command Performance"
            cat command_benchmarks.json | jq -r '.benchmarks[] | "- " + .name + ": " + (.stats.mean | tostring) + "s"'
            echo "## Processing Performance"
            cat processing_benchmarks.json | jq -r '.benchmarks[] | "- " + .name + ": " + (.stats.mean | tostring) + "s"'
          } > performance-report.md

  memory:
    name: Memory Benchmarks
    needs: prepare
    if: github.event.inputs.benchmark_type == 'memory' || github.event.inputs.benchmark_type == 'all'
    runs-on: ubuntu-latest

    steps:
      - name: Run memory profiling
        run: |
          python -m memory_profiler tests/benchmarks/test_memory_usage.py \
            --iterations ${{ github.event.inputs.iterations || 1000 }} \
            > memory_profile.txt

      - name: Run heap analysis
        run: |
          python tests/benchmarks/test_heap_usage.py \
            --iterations ${{ github.event.inputs.iterations || 1000 }} \
            --output heap_analysis.json

      - name: Generate memory report
        run: |
          {
            echo "# Memory Benchmark Report"
            echo "## Memory Profile"
            cat memory_profile.txt
            echo "## Heap Analysis"
            cat heap_analysis.json | jq -r '.measurements[] | "- " + .name + ": " + (.usage | tostring) + "MB"'
          } > memory-report.md

  latency:
    name: Latency Benchmarks
    needs: prepare
    if: github.event.inputs.benchmark_type == 'latency' || github.event.inputs.benchmark_type == 'all'
    runs-on: ubuntu-latest

    steps:
      - name: Run response time benchmarks
        run: |
          pytest tests/benchmarks/test_response_time.py \
            --benchmark-only \
            --benchmark-json response_benchmarks.json \
            --benchmark-min-rounds ${{ github.event.inputs.iterations || 1000 }}

      - name: Run network latency benchmarks
        run: |
          pytest tests/benchmarks/test_network_latency.py \
            --benchmark-only \
            --benchmark-json network_benchmarks.json \
            --benchmark-min-rounds ${{ github.event.inputs.iterations || 1000 }}

      - name: Generate latency report
        run: |
          {
            echo "# Latency Benchmark Report"
            echo "## Response Times"
            cat response_benchmarks.json | jq -r '.benchmarks[] | "- " + .name + ": " + (.stats.mean | tostring) + "ms"'
            echo "## Network Latency"
            cat network_benchmarks.json | jq -r '.benchmarks[] | "- " + .name + ": " + (.stats.mean | tostring) + "ms"'
          } > latency-report.md

  throughput:
    name: Throughput Benchmarks
    needs: prepare
    if: github.event.inputs.benchmark_type == 'throughput' || github.event.inputs.benchmark_type == 'all'
    runs-on: ubuntu-latest

    steps:
      - name: Run request throughput benchmarks
        run: |
          pytest tests/benchmarks/test_request_throughput.py \
            --benchmark-only \
            --benchmark-json request_benchmarks.json \
            --benchmark-min-rounds ${{ github.event.inputs.iterations || 1000 }}

      - name: Run processing throughput benchmarks
        run: |
          pytest tests/benchmarks/test_processing_throughput.py \
            --benchmark-only \
            --benchmark-json processing_throughput.json \
            --benchmark-min-rounds ${{ github.event.inputs.iterations || 1000 }}

      - name: Generate throughput report
        run: |
          {
            echo "# Throughput Benchmark Report"
            echo "## Request Throughput"
            cat request_benchmarks.json | jq -r '.benchmarks[] | "- " + .name + ": " + (.stats.mean | tostring) + " req/s"'
            echo "## Processing Throughput"
            cat processing_throughput.json | jq -r '.benchmarks[] | "- " + .name + ": " + (.stats.mean | tostring) + " ops/s"'
          } > throughput-report.md

  analyze:
    name: Analyze Results
    needs: [prepare, performance, memory, latency, throughput]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Compare with baseline
        run: |
          python scripts/benchmarks/compare_baseline.py \
            --current "*.json" \
            --baseline baseline/benchmarks/ \
            --output comparison.json

      - name: Analyze trends
        run: |
          python scripts/benchmarks/analyze_trends.py \
            --history benchmark_history.json \
            --current "*.json" \
            --output trends.json

      - name: Generate analysis report
        run: |
          {
            echo "# Benchmark Analysis Report"
            echo "## Baseline Comparison"
            cat comparison.json | jq -r '.comparisons[] | "- " + .'
            echo "## Performance Trends"
            cat trends.json | jq -r '.trends[] | "- " + .'
          } > analysis-report.md

  report:
    name: Generate Report
    needs: [prepare, performance, memory, latency, throughput, analyze]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Combine reports
        run: |
          {
            echo "# Benchmark Test Report"
            echo "Test ID: ${{ needs.prepare.outputs.test_id }}"
            
            echo "## Performance Benchmarks"
            cat performance-report.md
            
            echo "## Memory Benchmarks"
            cat memory-report.md
            
            echo "## Latency Benchmarks"
            cat latency-report.md
            
            echo "## Throughput Benchmarks"
            cat throughput-report.md
            
            echo "## Analysis"
            cat analysis-report.md
            
            echo "## Recommendations"
            python scripts/benchmarks/generate_recommendations.py \
              --analysis analysis-report.md \
              --output recommendations.md
            cat recommendations.md
          } > benchmark-report.md

      - name: Create issue for regressions
        if: contains(needs.*.result, 'failure')
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('benchmark-report.md', 'utf8');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'ðŸ“Š Performance Regression Detected',
              body: report,
              labels: ['performance', 'regression']
            });

      - name: Update metrics
        run: |
          {
            echo "benchmark_completion $(date +%s)"
            echo "performance_score $(python scripts/benchmarks/calculate_score.py analysis-report.md)"
          } > benchmark_metrics.txt

          curl -X POST ${{ secrets.PROMETHEUS_PUSHGATEWAY }}/metrics/job/benchmarking \
            --data-binary "@benchmark_metrics.txt"

      - name: Notify team
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          title: "Benchmark Complete"
          description: |
            Test ID: ${{ needs.prepare.outputs.test_id }}
            Iterations: ${{ github.event.inputs.iterations || 1000 }}

            Check the workflow run for detailed results.
          color: ${{ contains(needs.*.result, 'failure') && '0xff0000' || '0x00ff00' }}
          username: "Benchmark Bot"
