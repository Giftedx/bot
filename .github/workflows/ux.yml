name: User Experience Testing

on:
  push:
    branches: [main, develop]
    paths:
      - "src/**"
      - "tests/ux/**"
      - ".github/workflows/ux.yml"
  pull_request:
    branches: [main]
  schedule:
    - cron: "0 0 * * 2" # Weekly on Tuesday
  workflow_dispatch: # Allow manual trigger
    inputs:
      test_scope:
        description: "UX test scope"
        required: true
        type: choice
        options:
          - all
          - commands
          - interactions
          - responsiveness
          - usability

jobs:
  command-testing:
    name: Command Experience
    if: github.event.inputs.test_scope == 'commands' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.8"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-asyncio pytest-timeout

      - name: Test command response times
        run: |
          python tests/ux/test_command_response.py \
            --output response_times.json

      - name: Test command success rates
        run: |
          python tests/ux/test_command_success.py \
            --output success_rates.json

      - name: Test help messages
        run: |
          python tests/ux/test_help_messages.py \
            --output help_quality.json

      - name: Generate command report
        run: |
          {
            echo "# Command Experience Report"
            echo "## Response Times"
            cat response_times.json | jq -r '.[] | "- " + .command + ": " + (.response_time | tostring) + "ms"'
            echo "## Success Rates"
            cat success_rates.json | jq -r '.[] | "- " + .command + ": " + (.success_rate | tostring) + "%"'
            echo "## Help Quality"
            cat help_quality.json | jq -r '.[] | "- " + .command + ": " + .score + "/10"'
          } > command-report.md

  interaction-testing:
    name: User Interaction
    if: github.event.inputs.test_scope == 'interactions' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Test conversation flows
        run: |
          python tests/ux/test_conversation_flows.py \
            --output conversation_flows.json

      - name: Test error handling
        run: |
          python tests/ux/test_error_handling.py \
            --output error_handling.json

      - name: Test user feedback
        run: |
          python tests/ux/test_user_feedback.py \
            --output user_feedback.json

      - name: Generate interaction report
        run: |
          {
            echo "# User Interaction Report"
            echo "## Conversation Flows"
            cat conversation_flows.json | jq -r '.[] | "- " + .scenario + ": " + .rating + "/10"'
            echo "## Error Handling"
            cat error_handling.json | jq -r '.[] | "- " + .error_type + ": " + .handling_score + "/10"'
            echo "## User Feedback"
            cat user_feedback.json | jq -r '.[] | "- " + .feature + ": " + .satisfaction + "/10"'
          } > interaction-report.md

  responsiveness-testing:
    name: Bot Responsiveness
    if: github.event.inputs.test_scope == 'responsiveness' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Test concurrent handling
        run: |
          python tests/ux/test_concurrent_usage.py \
            --output concurrency.json

      - name: Test load handling
        run: |
          python tests/ux/test_load_handling.py \
            --output load_handling.json

      - name: Test recovery time
        run: |
          python tests/ux/test_recovery.py \
            --output recovery_times.json

      - name: Generate responsiveness report
        run: |
          {
            echo "# Responsiveness Report"
            echo "## Concurrent Usage"
            cat concurrency.json | jq -r '.[] | "- " + .scenario + ": " + (.performance | tostring) + "ms"'
            echo "## Load Handling"
            cat load_handling.json | jq -r '.[] | "- " + .load_level + ": " + .handling_score + "/10"'
            echo "## Recovery Times"
            cat recovery_times.json | jq -r '.[] | "- " + .scenario + ": " + (.recovery_time | tostring) + "ms"'
          } > responsiveness-report.md

  usability-testing:
    name: Usability Testing
    if: github.event.inputs.test_scope == 'usability' || github.event.inputs.test_scope == 'all' || github.event_name != 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Test command discovery
        run: |
          python tests/ux/test_command_discovery.py \
            --output discovery.json

      - name: Test learning curve
        run: |
          python tests/ux/test_learning_curve.py \
            --output learning_curve.json

      - name: Test user satisfaction
        run: |
          python tests/ux/test_satisfaction.py \
            --output satisfaction.json

      - name: Generate usability report
        run: |
          {
            echo "# Usability Report"
            echo "## Command Discovery"
            cat discovery.json | jq -r '.[] | "- " + .feature + ": " + .discoverability + "/10"'
            echo "## Learning Curve"
            cat learning_curve.json | jq -r '.[] | "- " + .feature + ": " + .difficulty + "/10"'
            echo "## User Satisfaction"
            cat satisfaction.json | jq -r '.[] | "- " + .aspect + ": " + .satisfaction + "/10"'
          } > usability-report.md

  report:
    name: Generate UX Report
    needs:
      [
        command-testing,
        interaction-testing,
        responsiveness-testing,
        usability-testing,
      ]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Combine reports
        run: |
          {
            echo "# User Experience Test Report"
            
            echo "## Command Experience"
            cat command-report.md
            
            echo "## User Interaction"
            cat interaction-report.md
            
            echo "## Responsiveness"
            cat responsiveness-report.md
            
            echo "## Usability"
            cat usability-report.md
            
            echo "## Overall Scores"
            echo "- Command Experience: $(jq -s 'add/length' command-report.md)/10"
            echo "- User Interaction: $(jq -s 'add/length' interaction-report.md)/10"
            echo "- Responsiveness: $(jq -s 'add/length' responsiveness-report.md)/10"
            echo "- Usability: $(jq -s 'add/length' usability-report.md)/10"
          } > ux-report.md

      - name: Create issue for improvements
        if: contains(needs.*.result, 'failure') || github.event_name == 'schedule'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('ux-report.md', 'utf8');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'üéØ UX Improvement Opportunities',
              body: report,
              labels: ['ux', 'enhancement']
            });

      - name: Update metrics
        run: |
          {
            echo "ux_tests_total $(date +%s)"
            echo "ux_score $(jq -s 'add/length' *-report.md)"
          } > ux_metrics.txt

          curl -X POST ${{ secrets.PROMETHEUS_PUSHGATEWAY }}/metrics/job/ux_testing \
            --data-binary "@ux_metrics.txt"

      - name: Notify team
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          title: "UX Tests Complete"
          description: |
            Status: ${{ contains(needs.*.result, 'failure') && '‚ùå Improvements Needed' || '‚úÖ All Tests Passed' }}

            Check the workflow run for detailed results.
          color: ${{ contains(needs.*.result, 'failure') && '0xff0000' || '0x00ff00' }}
          username: "UX Bot"
